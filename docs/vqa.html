<!DOCTYPE html>
<html  >
<head>
  <!-- Site made with Mobirise Website Builder v5.9.6, https://mobirise.com -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Mobirise v5.9.6, mobirise.com">
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:image:src" content="assets/images/vqa-meta.png">
  <meta property="og:image" content="assets/images/vqa-meta.png">
  <meta name="twitter:title" content="VQA Feedback Planning">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
  <link rel="shortcut icon" href="assets/images/logo-128x109.png" type="image/x-icon">
  <meta name="description" content="Portfolio:
I am Bhushan. ‚Äì a robotics graduate student currently pursuing my master's degree at Georgia Tech! üêù
This is a short overview of my work for Agile Systems lab @GT">
  
  
  <title>VQA Feedback Planning</title>
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons2/mobirise2.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-grid.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-reboot.min.css">
  <link rel="stylesheet" href="assets/parallax/jarallax.css">
  <link rel="stylesheet" href="assets/animatecss/animate.css">
  <link rel="stylesheet" href="assets/dropdown/css/style.css">
  <link rel="stylesheet" href="assets/socicon/css/styles.css">
  <link rel="stylesheet" href="assets/theme/css/style.css">
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i&display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i&display=swap"></noscript>
  <link rel="preload" as="style" href="assets/mobirise/css/mbr-additional.css"><link rel="stylesheet" href="assets/mobirise/css/mbr-additional.css" type="text/css">

  
  
  
</head>
<body>
  
  <section data-bs-version="5.1" class="menu menu2 cid-umFF1RCPGs" once="menu" id="menu2-6x">
    
    <nav class="navbar navbar-dropdown navbar-fixed-top navbar-expand-lg">
        <div class="container">
            <div class="navbar-brand">
                
                <span class="navbar-caption-wrap"><a class="navbar-caption text-primary display-7" href="index.html">Bhushan Pawaskar<br></a></span>
            </div>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbarSupportedContent" data-bs-target="#navbarSupportedContent" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
                <div class="hamburger">
                    <span></span>
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-dropdown nav-right" data-app-modern-menu="true"><li class="nav-item"><a class="nav-link link text-black text-primary display-4" href="index.html">Home</a></li>
                    <li class="nav-item dropdown"><a class="nav-link link text-black text-primary dropdown-toggle display-4" href="about.html" data-toggle="dropdown-submenu" data-bs-toggle="dropdown" data-bs-auto-close="outside" aria-expanded="false">About</a><div class="dropdown-menu" aria-labelledby="dropdown-613"><a class="text-black text-primary dropdown-item display-4" href="about.html#header11-l">Intro</a><a class="text-black text-primary dropdown-item display-4" href="about.html#pricing2-w">Skills</a><a class="text-black text-primary dropdown-item display-4" href="about.html#timeline3-s">Education</a></div></li><li class="nav-item dropdown"><a class="nav-link link text-black text-primary dropdown-toggle show display-4" href="projects.html" data-toggle="dropdown-submenu" data-bs-toggle="dropdown" data-bs-auto-close="outside" aria-expanded="false">Projects</a><div class="dropdown-menu show" aria-labelledby="dropdown-358" data-bs-popper="none"><a class="text-black text-primary dropdown-item display-4" href="projects.html#slider4-15">Computer Vision</a><a class="text-black text-primary dropdown-item display-4" href="projects.html#slider4-16">Control &amp; Planning</a><a class="text-black text-primary dropdown-item display-4" href="projects.html#slider04-14">Live Demos</a><a class="text-black text-primary dropdown-item display-4" href="projects.html#slider4-17">Mechanical</a><a class="text-black text-primary dropdown-item display-4" href="projects.html#slider4-10">Miscellaneous</a></div></li></ul>
                
                
            </div>
        </div>
    </nav>
</section>

<section data-bs-version="5.1" class="article4 cid-umFF1RVQN3 mbr-parallax-background" id="article04-6y">
	

	
	

	<div class="container">
		<div class="row justify-content-center">
			<div class="col-12 col-md-12 col-lg-5 image-wrapper">
				<img class="w-100" src="assets/images/manipulator-320x180.webp" alt="">
			</div>
			<div class="col-12 col-md-12 col-lg">
				<div class="text-wrapper align-left">
					<h1 class="mbr-section-title mbr-fonts-style mb-4 display-2">
						<strong>Introduction</strong></h1>
					<p class="mbr-text mbr-fonts-style mb-4 display-7">This project was done as a class project for Deep Learning for Robotics class&nbsp;@Georgia Tech.<br><br>While humans naturally follow an act-see-reason-act paradigm, current robot <strong>planning approaches often lack the ability to incorporate feedback</strong> from actions and recover from failures. This sort of long horizon task planning is missing in robots.<br><br>In this project, we propose a novel approach that <strong>combines large language models (LLMs) with a visual reasoning system (VRS)</strong> inspired by the human visual cortex.<br></p>
					<div class="mbr-section-btn mt-3"><a class="btn btn-lg btn-primary display-7" href="assets/files/Closed_Loop_LLM_Based_Planning_and_Execution_with_Visual_Reasoning.pdf" target="_blank"><span class="mobi-mbri mobi-mbri-file mbr-iconfont mbr-iconfont-btn"></span>Project Overview&nbsp;</a></div>
				</div>
			</div>
		</div>
	</div>
</section>

<section data-bs-version="5.1" class="article4 cid-umFF1SdkFC" id="article04-6z">
	

	
	<div class="mbr-overlay" style="opacity: 0.7; background-color: rgb(255, 255, 255);"></div>

	<div class="container">
		<div class="row justify-content-center">
			<div class="col-12 col-md-12 col-lg-5 image-wrapper">
				<img class="w-100" src="assets/images/system-flowchart-1036x584.webp" alt="">
			</div>
			<div class="col-12 col-md-12 col-lg">
				<div class="text-wrapper align-left">
					<h1 class="mbr-section-title mbr-fonts-style mb-4 display-2">
						<strong>Goals</strong></h1>
					<p class="mbr-text mbr-fonts-style mb-4 display-7">Consider a discretized system of n steps from start state s0 to goal state g(L). We consider the problem of <strong>embodied task planning where an agent needs to generate a sequence of </strong>n discrete low level actions ak, in order to <strong>accomplish a high-level natural language instruction L given in english like ‚ÄùPlace all fruits in the bowl‚Äù.</strong><br><br>The agent has access to observations ok of the world state sk, at the start of every action ak, in the form of RGB image inputs. The agent has to find a policy function œÄ such that, given an instruction L, the function maps the observation space to actions that drive the state toward sn = g(L)<br><br>The system chosen was based on <strong><a href="https://github.com/google-research/ravens" class="text-primary" target="_blank">Ravens simulator</a></strong> doing pick and place tabletop tasks. The goal was to <strong>make the robotic manipulator perform tasks like placing a stack of blocks</strong>&nbsp;on top of each other or <strong>putting blocks in bowls</strong>.</p>
					
				</div>
			</div>
		</div>
	</div>
</section>

<section data-bs-version="5.1" class="article4 cid-umFF1SvnSx mbr-parallax-background" id="article04-70">
	

	
	

	<div class="container">
		<div class="row justify-content-center">
			<div class="col-12 col-md-12 col-lg-5 image-wrapper">
				<img class="w-100" src="assets/images/agnet-submodules-flowchart-1036x581.webp" alt="">
			</div>
			<div class="col-12 col-md-12 col-lg">
				<div class="text-wrapper align-left">
					<h1 class="mbr-section-title mbr-fonts-style mb-4 display-2">
						<strong>Work Done</strong></h1>
					<p class="mbr-text mbr-fonts-style mb-4 display-7">Our system consists of <strong>4 parts:</strong><br>1. <strong>Controller</strong> - Central hub to co-ordinate actions<br>2. <strong>Actor</strong> - Executes low level actions<br>3. <strong>VRS </strong>- Visual Reasoning to sense the system<br>4. <strong>Planner</strong> - Makes code-plans to break down the action<br><br>The <strong>controller was just a locally executed script</strong> that timed each action with logic given predetermined checks. The <strong>actor was directly taken from <a href="https://cliport.github.io/" class="text-primary" target="_blank">CLIPORT</a></strong> which combines <a href="https://openai.com/index/clip/" class="text-primary" target="_blank">CLIP encodings</a> and <a href="https://transporternets.github.io/" class="text-primary" target="_blank">Transporter networks</a> to execute intelligent actions from textual embeddings. The <strong>code-plan was generated using LLMs</strong> to break down the larger task into a simplistic one. While multimodal <strong><a href="https://github.com/OFA-Sys/OFA" class="text-primary" target="_blank">OFA model</a> was used for the Visual Reasoning module</strong>.</p>
					
				</div>
			</div>
		</div>
	</div>
</section>

<section data-bs-version="5.1" class="article4 cid-umFF1SLQpS" id="article04-71">
	

	
	<div class="mbr-overlay" style="opacity: 0.7; background-color: rgb(255, 255, 255);"></div>

	<div class="container">
		<div class="row justify-content-center">
			<div class="col-12 col-md-12 col-lg-5 image-wrapper">
				<img class="w-100" src="assets/images/method-1036x636.webp" alt="">
			</div>
			<div class="col-12 col-md-12 col-lg">
				<div class="text-wrapper align-left">
					<h1 class="mbr-section-title mbr-fonts-style mb-4 display-2"></h1>
					<p class="mbr-text mbr-fonts-style mb-4 display-7">My task was involved with <strong>finetuning the OFA module</strong>. It seemed to struggle with complex spatial tasks related to multiple objects for our data even when it had high accuracy on VQAV2 benchmark.<br><br>Generating training data was a challenge as we required a way to <strong>caption each image with multiple questions and corresponding answers</strong>. We decided to approach this using the <strong>CLEVR dataset generation pipeline which is based on top of the Blender API</strong>. This generates both random images with various spatial configurations and corresponding question and answers for the training.<br><br><br></p>
					
				</div>
			</div>
		</div>
	</div>
</section>

<section data-bs-version="5.1" class="article4 cid-umFF1T2UKT mbr-parallax-background" id="article04-72">
	

	
	

	<div class="container">
		<div class="row justify-content-center">
			<div class="col-12 col-md-12 col-lg-5 image-wrapper">
				<img class="w-100" src="assets/images/results-1036x636.webp" alt="">
			</div>
			<div class="col-12 col-md-12 col-lg">
				<div class="text-wrapper align-left">
					<h1 class="mbr-section-title mbr-fonts-style mb-4 display-2"><strong>Results</strong></h1>
					<p class="mbr-text mbr-fonts-style mb-4 display-7">We generated over <strong>1000 images and 5000+ corresponding QnA</strong> by modifying the pipeline for our use case. This involved multiple combination of block colors, bowl colors, stack sizes, spatial placements. The questions also had variety using <strong>5-6 general question templates</strong> that challenged the model to generalize well.<br><br>We finetuned the model on a weighted mixture of this dataset and VQAV2 and the resulting model obtained <strong>about 50% accuracy on our tasks</strong> which was comparable to benchmark accuracies. (Earlier it was 0%)<br></p>
					
				</div>
			</div>
		</div>
	</div>
</section>

<section data-bs-version="5.1" class="article4 cid-umFF1ThYCE" id="article04-73">
	

	
	<div class="mbr-overlay" style="opacity: 0.7; background-color: rgb(255, 255, 255);"></div>

	<div class="container">
		<div class="row justify-content-center">
			<div class="col-12 col-md-12 col-lg-5 image-wrapper">
				<img class="w-100" src="assets/images/steps-752x527.webp" alt="">
			</div>
			<div class="col-12 col-md-12 col-lg">
				<div class="text-wrapper align-left">
					<h1 class="mbr-section-title mbr-fonts-style mb-4 display-2"></h1>
					<p class="mbr-text mbr-fonts-style mb-4 display-7">For the final part we combine the feedback with our Codeplan module and compare the results without it. We test it for <strong>3 different tasks over a series of 15 iterations of experiments</strong>. Retries more than 20 count as a failure for the task. We notice that there was a <strong>considerable increase in success</strong> when using our method when the robot has Visual Feedback to correct for its wrong actions. <br><br>By following a cyclic three-stage process of breaking tasks down into single-step sub-tasks, executing these sub-tasks while generating feedback, and dynamically re-planning when necessary, our method <strong>empowers robots to recover swiftly from failures</strong> and successfully accomplish long-term tasks<br></p>
					
				</div>
			</div>
		</div>
	</div>
</section>

<section data-bs-version="5.1" class="contacts2 cid-umFF1TxkOT" id="contacts2-74">
    <!---->
    
    
    <div class="container">
        <div class="mbr-section-head">
            <h3 class="mbr-section-title mbr-fonts-style align-center mb-0 display-2"><strong>Contact me!</strong></h3>
            
        </div>
        <div class="row justify-content-center mt-4">
            <div class="card col-12 col-md-6">
                <div class="card-wrapper">
                    <div class="image-wrapper">
                        <span class="mbr-iconfont socicon-linkedin socicon"></span>
                    </div>
                    <div class="text-wrapper">
                        <h6 class="card-title mbr-fonts-style mb-1 display-5">
                            <strong>Linkedin</strong>
                        </h6>
                        <p class="mbr-text mbr-fonts-style display-7"><a href="http://linkedin.com/in/bhushan-p " class="text-primary">linkedin.com/in/bhushan-p&nbsp;</a></p>
                    </div>
                </div>
            </div>
            <div class="card col-12 col-md-6">
                <div class="card-wrapper">
                    <div class="image-wrapper">
                        <span class="mbr-iconfont socicon-github socicon"></span>
                    </div>
                    <div class="text-wrapper">
                        <h6 class="card-title mbr-fonts-style mb-1 display-5">
                            <strong>Github</strong></h6>
                        <p class="mbr-text mbr-fonts-style display-7"><a href="http://github.com/bhushanap" class="text-primary" target="_blank">github.com/bhushanap</a></p>
                    </div>
                </div>
            </div>
            
            
        </div>
    </div>
</section>

<section data-bs-version="5.1" class="footer7 cid-umFF1TT3OJ" once="footers" id="footer7-75">

    

    

    <div class="container">
        <div class="media-container-row align-center mbr-white">
            <div class="col-12">
                <p class="mbr-text mb-0 mbr-fonts-style display-4"></p><p>üìû<a href="tel:+1-404-3883944" class="text-white">(404) 388-3944</a>&nbsp;&nbsp;&nbsp;&nbsp;üìçAtlanta, GA, 30309&nbsp; &nbsp;üìß<a href="mailto:bhushan.pawaskar@hotmail.com" class="text-white">bhushan.pawaskar@hotmail.com</a><br><br>¬© Copyright 2023 Bhushan Pawaskar - All Rights Reserved
                </p><p></p>
            </div>
        </div>
    </div>
</section><section class="display-7" style="padding: 0;align-items: center;justify-content: center;flex-wrap: wrap;    align-content: center;display: flex;position: relative;height: 4rem;"><img alt="" style="height: 4rem;" src="data:image/gif;base64,R0lGODlhAQABAIAAAP///wAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw=="></a><p style="margin: 0;text-align: center;" class="display-7">&#8204;</p><a style="z-index:1" href="https://mobirise.com/builder/ai-website-builder.html"></a></section><script src="assets/bootstrap/js/bootstrap.bundle.min.js"></script>  <script src="assets/parallax/jarallax.js"></script>  <script src="assets/smoothscroll/smooth-scroll.js"></script>  <script src="assets/ytplayer/index.js"></script>  <script src="assets/dropdown/js/navbar-dropdown.js"></script>  <script src="assets/theme/js/script.js"></script>  
  
  
 <div id="scrollToTop" class="scrollToTop mbr-arrow-up"><a style="text-align: center;"><i class="mbr-arrow-up-icon mbr-arrow-up-icon-cm cm-icon cm-icon-smallarrow-up"></i></a></div>
    <input name="animation" type="hidden">
  </body>
</html>